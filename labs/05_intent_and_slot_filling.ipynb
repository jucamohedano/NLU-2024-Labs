{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca7d9e5",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "<!-- - pytorch \n",
    "    - Pytorch install: https://pytorch.org/get-started/locally/ \n",
    "- tqdm\n",
    "- sklearn\n",
    "- Huggingface Transformer: \n",
    "    - pip install transformers  -->\n",
    "- **DATASET**:\n",
    "    - https://github.com/BrownFortress/IntentSlotDatasets\n",
    "    - We will use **ATIS** only\n",
    "    \n",
    "    \n",
    "\n",
    "## Outline\n",
    "\n",
    "#### Introduction\n",
    "- sequence labelling (Slot filling)\n",
    "- text classification (Intent classification)\n",
    "\n",
    "#### Preparing text for NN\n",
    "- word2id\n",
    "- special tokens \n",
    "- Customize Dataset class\n",
    "\n",
    "#### Split data in batches\n",
    "- Usage of Dataloader class\n",
    "- Padding sequences\n",
    "\n",
    "#### Neural Networks in Pytorch\n",
    "- Word embeddings\n",
    "- Implementation of an LSTM\n",
    "- Regularization techniques\n",
    "\n",
    "#### Train and Test a Neural Network\n",
    "- Optimizer\n",
    "- Loss function\n",
    "- Iteration over batches\n",
    "\n",
    "#### Hugging face library\n",
    "- Introduction and Usage\n",
    " \n",
    " \n",
    "## References\n",
    "- RNN: https://d2l.ai/chapter_recurrent-neural-networks/index.html \n",
    "- LSTM: https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "- GRU: https://d2l.ai/chapter_recurrent-modern/gru.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0b990",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brownfortress/NLU-2024-labs/blob/main/labs/05_intent_and_slot_filling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a0554",
   "metadata": {},
   "source": [
    "# 1 Sequence Labeling,  Shallow Parsing and Text classification tasks\n",
    "\n",
    "## 1.1 Sequence Labeling and Shallow parsing\n",
    "Sequence labelling is to assign a label for each token. The task is formally defined as:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- defining a sequence of labels as $l = {l_1, l_2, ..., l_n}$\n",
    "- compute the sequence $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "A particular case of sequence labelling is [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing). The main difference from Sequence Labeling task is that Shallow Parsing performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "\n",
    "In this, we are going to see a particular case of shallow parsing task, which is named as Slot Filling (or Concept tagging). The **segmentation** part is represented with IOB tags and the **labeling** part are the concepts defined in the annotation schema of a corpus. \\\n",
    "\\\n",
    "An example is the following: \n",
    "\n",
    "| Slot Filling |  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output sequence: | O  | B-depart_date.month_name | B-depart_date.day_number | O | O    | O | O      |\n",
    "\n",
    "## 1.2 Text classification\n",
    "The text classification problem is defined  as follows:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- And a set of labels $L$ where $l \\in L$\n",
    "- estimate the label $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "In text classification, the label is given to the whole input sequence instead of at each element of the sequence (as in sequence labelling).\n",
    "\n",
    "The text classification task that we are going to see in this laboratory is named as Intent Classification. The Intent is an additional component of the *semantic frame*. \\\n",
    "\\\n",
    "An example is the following:\n",
    "\n",
    "| Intent Classification|  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output label: | flight     |\n",
    "\n",
    "\n",
    "# 2 Dataset\n",
    "The dataset that we are going to use is ATIS (Airline Travel Information Systems). It is composed of transcriptions of humans asking about flight information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3289e",
   "metadata": {},
   "source": [
    "## 2.2 Load the dataset\n",
    "I have prepared a custom data structure for this dataset. The structure is the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "    \"utterance\": \"on april first i need a flight going from phoenix to san diego\", \n",
    "    \"slots\": \"O B-depart_date.month_name B-depart_date.day_number O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name\", \n",
    "    \"intent\": \"flight\"\n",
    "    },\n",
    "    \"...\"\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70494a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Colab, run these commands\n",
    "# !wget -P dataset/ATIS https://raw.githubusercontent.com/BrownFortress/IntentSlotDatasets/main/ATIS/test.json\n",
    "# !wget -P dataset/ATIS https://raw.githubusercontent.com/BrownFortress/IntentSlotDatasets/main/ATIS/train.json\n",
    "# !wget https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/conll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80808524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "import os\n",
    "device = 'cpu' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e1ea7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "{'intent': 'flight',\n",
      " 'slots': 'O O O O O B-fromloc.city_name O B-depart_time.time '\n",
      "          'I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O '\n",
      "          'B-arrive_time.period_of_day',\n",
      " 'utterance': 'i want to fly from boston at 838 am and arrive in denver at '\n",
      "              '1110 in the morning'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('dataset','ATIS','train.json'))\n",
    "test_raw = load_data(os.path.join('dataset','ATIS','test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "pprint(tmp_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b3f37",
   "metadata": {},
   "source": [
    "## 2.3 Create a dev set\n",
    "In the original split the development set (dev set) is missing. To train and find the best hyperparameter of our network the dev set is fundamental. Thus, we have to create it starting from the **traning** set. The dev set is usually the 10% of the dataset. \\\n",
    "Possible sampling strategies:\n",
    "* Take the last n elements of the training set.\n",
    "* Do a random sampling from the training set.\n",
    "* Do a stratified sampling from the training set using one or more criteria. (The best way)\n",
    "    * For further details look [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccd6da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.4,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.4,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.0,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4480\n",
      "DEV size: 498\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# First we get the 10% of the training set, then we compute the percentage of these examples \n",
    "\n",
    "portion = 0.10\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "labels = []\n",
    "inputs = []\n",
    "mini_train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occurs only once, we put them in training\n",
    "        inputs.append(tmp_train_raw[id_y])\n",
    "        labels.append(y)\n",
    "    else:\n",
    "        mini_train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(inputs, labels, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=labels)\n",
    "X_train.extend(mini_train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distributions\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbdbe",
   "metadata": {},
   "source": [
    "## 2.3 Convert words to numbers (word2id)\n",
    "Neural Networks in Pytorch, as in other libraries, work with numbers and vectors.\n",
    "<br><br>\n",
    "\n",
    "**Exercise 1** *(10 minutes)*\n",
    "* Create a dictionary that maps the words and labels in the training set to unique  integers $\\geq$ 0, called indexes.\n",
    "    That is:\n",
    "    - One dictionary for mapping words to ids (w2id)\n",
    "    - One dictionary for mapping slot labels to ids (slot2id)\n",
    "    - One dictionary for mapping intent labels to ids (intent2id)\n",
    "\n",
    "* With w2id map the sentence in `sent` into the computed indexes.\n",
    "\n",
    "***Example:***\n",
    "```python\n",
    "dictionary = {\"from\": 2, \"Boston\":88, \"to\":105, \"Tokyo\":42}\n",
    "sent = \"from Boston to Tokyo\" \n",
    "# Output:\n",
    "[2,88,105,42]\n",
    "```\n",
    "\n",
    "We will see later how to convert these indexes into vectors (aka embeddings).\n",
    "\n",
    "*Add special tokens \"pad\" and \"unk\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea03b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "# Vocab: -1\n",
      "# Slots: 0\n",
      "# Intent: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "w2id = {'pad':PAD_TOKEN} # Pad tokens is 0 so the index count should start from 1\n",
    "slot2id = {'pad':PAD_TOKEN} # Pad tokens is 0 so the index count should start from 1\n",
    "intent2id = {}\n",
    "\n",
    "# Map the words only from the train set\n",
    "# Map slot and intent labels of train, dev and test set. 'unk' is not needed.\n",
    "sent = 'I wanna a flight from Toronto to Kuala Lumpur'\n",
    "\n",
    "mapping = [] # convert the sent into indexes using w2id \n",
    "print(mapping)\n",
    "\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be408b",
   "metadata": {},
   "source": [
    "## 2.4 Lang class\n",
    "Later we will need to convert those numbers in the original form, so we need to invert those dictionaries. We create a class named as Lang just for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c04f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77bc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw # We do not wat unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "lang = Lang(words, intents, slots, cutoff=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac746e",
   "metadata": {},
   "source": [
    "## 2.5 Customize the Dataset class\n",
    "In Pytorch the Dataset class helps you in handeling the dataset. The mandatory methods are ```__init__, __len__ and __getitem__```. <br>\n",
    "You can find more details here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b4823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class IntentsAndSlots (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "845ab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our datasets\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297a312",
   "metadata": {},
   "source": [
    "# 3 Batches\n",
    "Batches are used to handle large datasets in the memory. Since the whole dataset cannot fit in GPU memories, we randomly shuffle the dataset and we split it in small batches that will be processed one at a time.\n",
    "## 3.1 Padding\n",
    "Padding is a strategy to fit sequences of different lengths into a matrix. For instance:\n",
    "\n",
    "| Right padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| book | me | a | flight | [pad] | [pad] | [pad] | \n",
    "\n",
    "| Left padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| [pad] | [pad] | [pad] | book | me | a | flight | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eb4c4",
   "metadata": {},
   "source": [
    "**Exercise 2** *(5 minutes)* <br> \n",
    "Write a function that adds padding on the right. (No need to convert the sentences to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71bb2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split them by white space\n",
    "sequences = ['I saw a man with a telescope', \n",
    "             'book me a flight', \n",
    "             'I want to see the flights from Milan to Ibiza']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b71d2",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "To split the dataset into batches and add padding we will use the DataLoader class. \n",
    "```python\n",
    "DataLoader(Dataset, batch_size=N, collate_fn={custom function}, shuffle=True)\n",
    "```\n",
    "*collate_fn* is used to shape the output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7105180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    # print(len(data[0]))\n",
    "    # print(data[0].keys())\n",
    "    # print(data[0][\"utterance\"])\n",
    "\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "        \n",
    "    # print(len(new_item))\n",
    "    # print([d[key] for d in data])\n",
    "    # print(new_item[\"utterance\"])\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our selected device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiations\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e459effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization example for the data in the previous cell\n",
    "# For help. Right before calling merge, this is how the data dictionaries that we have look like.\n",
    "data = [\n",
    "    {'utterance': ['Hello'], 'slots': ['Greeting'], 'intent': 'greet'},\n",
    "    {'utterance': ['How are you?'], 'slots': ['Greeting'], 'intent': 'greet'},\n",
    "    {'utterance': ['Goodbye'], 'slots': ['Farewell'], 'intent': 'farewell'}\n",
    "]\n",
    "\n",
    "# After running the for loop we get:\n",
    "new_item = {\n",
    "    'utterance': [['Hello'], ['How are you?'], ['Goodbye']],\n",
    "    'slots': [['Greeting'], ['Greeting'], ['Farewell']],\n",
    "    'intent': ['greet', 'greet', 'farewell']\n",
    "}\n",
    "# then the next step would be merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019e479",
   "metadata": {},
   "source": [
    "# 4 Define a neural network in Pytorch\n",
    "In PyTorch the definition of a neural network is quite flexible. In ```__init__``` the layer that is going to be used are instantiated. In ```forward```, the architecture of the neural network is defined. Here you can find all the layers provided by Pytorch https://pytorch.org/docs/stable/nn.html while here you can find the recurrent layers https://pytorch.org/docs/stable/nn.html#recurrent-layers. \n",
    "\n",
    "<br><br>\n",
    "**pack_padded_sequence** and **pad_packed_sequences** respectively compress and uncompress sequences to remove the padding embeddings from the computation, reducing the computational cost and, therefore, the CO2 emissions.\n",
    " ![](https://i.stack.imgur.com/LPHAs.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93adc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (output size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False, batch_first=True)\n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        # Dropout layer How/Where do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        \n",
    "        utt_emb = self.dropout(utt_emb) # we can use dropout after the embedding layer\n",
    "\n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        #.cpu().numpy() converts the seq_lengths tensor to a NumPy array and moves it to the CPU memory\n",
    "        # This is done because pack_padded_sequence expects the sequence lengths to be provided as a CPU-based NumPy array.\n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "\n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        utt_emb = self.dropout(utt_encoded) # we can use dropout after the LSTM layer too!\n",
    "        \n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        \n",
    "        # Is this another possible way to get the last hiddent state? (Why?)\n",
    "        # utt_encoded.permute(1,0,2)[-1]\n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: batch_size, seq_len, classes \n",
    "        slots = slots.permute(0,2,1) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a1ecc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Why do we pad the sequences twice? \n",
    "  - First time in the merge function above\n",
    "  - Second time in the forward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992a22c",
   "metadata": {},
   "source": [
    "## 3.1 Function to randomly initialize the weights\n",
    "This is a generic function that randomly initialize the parameters of RNN networks and linear layers. To dig deep in to this I would suggest you to look at here: https://pytorch.org/docs/master/nn.init.html \\\n",
    "\\\n",
    "*Note: In Pytorch every parameter of the network has a proper name like weight_ih, weight_hh etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f47fe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a362fc",
   "metadata": {},
   "source": [
    "## 3.2 Training set up\n",
    "We initialize the model and we select the hyperparameters of the neural network. Futhermore, we initialize the optimizer and we select the loss function.\n",
    "- You can find further optimization algorithms here: https://pytorch.org/docs/stable/optim.html\n",
    "- and further loss functions here: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e5edf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a18f4",
   "metadata": {},
   "source": [
    "### Train Loop and Evaluation Loop\n",
    "We define two functions one for training our model and the other for evaluating it. To compute the performances on the slot filling task we will use the **conll script**, while for the intent classification task we are going to use the **classification_report**.\n",
    "\n",
    "<br>\n",
    "\n",
    "In the literature, the Intent Classification task is evaluated using accuracy as a metric. The Slot filling task is evaluated using the *conll script* which computes the performance at the chunk level and the F1 score is usually reported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef292a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### <font color='blue'>Note on gradient clipping</font>\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(...): This function clips the gradients of the model's parameters. It ensures that the norm of the gradients does not exceed the specified clip value.\n",
    "Gradient Clipping:\n",
    "For each parameter tensor in the model, the function computes the L2-norm (Euclidean norm) of the gradient.\n",
    "If the norm of the gradient exceeds the clip value, the gradient tensor is rescaled to have a norm equal to clip.\n",
    "In the provided code, torch.nn.utils.clip_grad_norm_() is used to ensure that the L2-norm of the gradients does not exceed the specified clip value, effectively limiting the maximum allowed gradient magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6bf6dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_loop(data, optimizer, criterion_slots, criterion_intents, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference \n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predicts a class that is not in REF\n",
    "        print(\"Warning:\", ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        results = {\"total\":{\"f\":0}}\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2eaee",
   "metadata": {},
   "source": [
    "## 3.3 Train a neural network\n",
    "We train a neural network iterating several times over the training set. \n",
    "* **epochs**: number of times in which the whole training set is seen by the network\n",
    "* **early stopping**: keeps controlled the performance of the model on the dev set and interrupts the training when the performance is getting worse\n",
    "    * **patience**: wait for a number of step before interrupting the training, even though the performance is getting worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d07777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/199 [00:00<?, ?it/s]/home/juancm/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  5%|â–         | 9/199 [00:12<04:24,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,n_epochs)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_loop(train_loader, optimizer, criterion_slots, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                       criterion_intents, model, clip\u001b[39m=\u001b[39;49mclip)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# We check the performance every 5 epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         sampled_epochs\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 35\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss_array \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Zeroing the gradient\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     slots, intent \u001b[39m=\u001b[39m model(sample[\u001b[39m'\u001b[39m\u001b[39mutterances\u001b[39m\u001b[39m'\u001b[39m], sample[\u001b[39m'\u001b[39m\u001b[39mslots_len\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     new_item[key] \u001b[39m=\u001b[39m [d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# print(len(new_item))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print([d[key] for d in data])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# print(new_item[\"utterance\"])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# We just need one length for packed pad seq, since len(utt) == len(slots)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m src_utt, _ \u001b[39m=\u001b[39m merge(new_item[\u001b[39m'\u001b[39;49m\u001b[39mutterance\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m y_slots, y_lengths \u001b[39m=\u001b[39m merge(new_item[\u001b[39m\"\u001b[39m\u001b[39mslots\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m intent \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(new_item[\u001b[39m\"\u001b[39m\u001b[39mintent\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, seq \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sequences):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     end \u001b[39m=\u001b[39m lengths[i]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     padded_seqs[i, :end] \u001b[39m=\u001b[39m seq \u001b[39m# We copy each sequence into the matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print(padded_seqs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m padded_seqs \u001b[39m=\u001b[39m padded_seqs\u001b[39m.\u001b[39mdetach()  \u001b[39m# We remove these tensors from the computational graph\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc39e9c",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "To save the model you have to save:\n",
    "- The weights of the model\n",
    "- The computed vocabularies (w2id, slot2id, intent2id)\n",
    "- The optimizer (optionally, only if you want to continue with the training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = os.path.join(\"bin\", model_name)\n",
    "# saving_object = {\"epoch\": x, \n",
    "#                  \"model\": model.state_dict(), \n",
    "#                  \"optimizer\": optimizer.state_dict(), \n",
    "#                  \"w2id\": w2id, \n",
    "#                  \"slot2id\": slot2id, \n",
    "#                  \"intent2id\": intent2id}\n",
    "# torch.save(saving_object, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1466a",
   "metadata": {},
   "source": [
    "### Plot of the train and valid losses\n",
    "One of the techniques for debugging a neural network is to check the plot of the loss. If the loss goes smoothly down then the network works correctly, otherwise a deeper analysis is needed. Furthermore, this plot can be useful for deciding the learning rate and the optimizer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1211aab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
    "plt.title('Train and Dev Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(sampled_epochs, losses_train, label='Train loss')\n",
    "plt.plot(sampled_epochs, losses_dev, label='Dev loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df73c",
   "metadata": {},
   "source": [
    "### Multiple runs\n",
    "To have reliable results on small corpora we have to train and test the model from scratch for several times. At the end, we average the results and we compute the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f08d0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 41\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,n_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_loop(train_loader, optimizer, criterion_slots, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                       criterion_intents, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         sampled_epochs\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 41\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Zeroing the gradient\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     slots, intent \u001b[39m=\u001b[39m model(sample[\u001b[39m'\u001b[39;49m\u001b[39mutterances\u001b[39;49m\u001b[39m'\u001b[39;49m], sample[\u001b[39m'\u001b[39;49m\u001b[39mslots_len\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_intent \u001b[39m=\u001b[39m criterion_intents(intent, sample[\u001b[39m'\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss_slot \u001b[39m=\u001b[39m criterion_slots(slots, sample[\u001b[39m'\u001b[39m\u001b[39my_slots\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 41\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m packed_input \u001b[39m=\u001b[39m pack_padded_sequence(utt_emb, seq_lengths\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Process the batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m packed_output, (last_hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mutt_encoder(packed_input) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Unpack the sequence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m utt_encoded, input_sizes \u001b[39m=\u001b[39m pad_packed_sequence(packed_output, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/rnn.py:881\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    878\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    879\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    880\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[1;32m    883\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[1;32m    884\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "n_epochs = 200\n",
    "runs = 5\n",
    "\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stopping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4565d",
   "metadata": {},
   "source": [
    " ![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\n",
    "# Hugging Face\n",
    "Hugging Face is a library that allows you to use pretrained models in an easy way. This means that you do not need to implement an architecture and train it from scratch. Hugging Face is based on a community where people share trained models and code.\n",
    "<br/><br/>\n",
    "In Hugging Face there are many different models (https://huggingface.co/models) that you can import and each of them has its own input and output shapes. However, Transformer-based models are usually composed of two parts: \n",
    "- **Tokenizer**\n",
    "- **Architecture/Pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b01cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[  101,  1045,  2387,  1037,  2158,  2007,  1037, 12772,   102],\n",
      "        [  101,  2732, 19980,  2001,  2182,   102,     0,     0,     0],\n",
      "        [  101,  1045,  2134,  1005,  1056,   102,     0,     0,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# BERT model script from: huggingface.co\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Download the tokenizer\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") # Download the model\n",
    "\n",
    "inputs = tokenizer([\"I saw a man with a telescope\", \"StarLord was here\",  \"I didn't\"], return_tensors=\"pt\", padding=True)\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36f26671",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c56ea16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e708509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0899,  0.2947, -0.1465,  ..., -0.0741,  0.2065,  0.2654],\n",
       "         [ 0.2638, -0.6021,  0.0573,  ..., -0.2103,  0.5828, -0.0745],\n",
       "         [-0.1865, -0.1801, -0.2631,  ..., -0.2649, -0.0541, -0.5011],\n",
       "         ...,\n",
       "         [-0.0602,  0.1210, -0.1306,  ...,  0.0296,  0.2848,  0.4798],\n",
       "         [ 0.2517, -0.0991, -0.4019,  ...,  0.4821,  0.4933,  0.2647],\n",
       "         [ 0.9127,  0.3971, -0.5130,  ...,  0.0268, -0.4916, -0.3104]],\n",
       "\n",
       "        [[-0.1358,  0.2670,  0.0614,  ..., -0.2989,  0.2887,  0.0766],\n",
       "         [ 0.2482, -0.0614,  0.7976,  ..., -0.4689,  0.4788, -0.9119],\n",
       "         [-0.3823, -0.1494,  0.0288,  ..., -0.3129, -0.2709, -1.1628],\n",
       "         ...,\n",
       "         [ 0.0446,  0.1307,  0.2074,  ..., -0.0040, -0.0031, -0.1453],\n",
       "         [-0.1345, -0.1548,  0.2327,  ...,  0.2537,  0.1917, -0.1686],\n",
       "         [ 0.1085, -0.0564,  0.4243,  ...,  0.0429,  0.0492, -0.1676]],\n",
       "\n",
       "        [[ 0.2613,  0.1113, -0.0772,  ...,  0.1774,  0.1918,  0.1912],\n",
       "         [-0.1582, -0.0076,  0.0350,  ..., -0.4753,  0.7891,  0.1073],\n",
       "         [ 1.0183, -0.1164,  0.0733,  ...,  0.4179, -0.0071, -0.2541],\n",
       "         ...,\n",
       "         [ 0.2764, -0.1756, -0.0258,  ...,  0.2266,  0.1695,  0.0419],\n",
       "         [ 0.1465, -0.2760, -0.1415,  ...,  0.4618,  0.2422, -0.1738],\n",
       "         [ 0.4087, -0.0733, -0.0888,  ...,  0.3673,  0.0205, -0.3218]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4a94c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs[0] == outputs.last_hidden_state\n",
    "torch.isclose(outputs[0], outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9139e1c",
   "metadata": {},
   "source": [
    "##  Byte pair encoding\n",
    "The tricky part of using Transformer-based model is the tokenizer which is based on byte pair encoding algorithm. Indeed, the **tokenizers** used by Transformer-based models are different from those we have seen in the lab. While for instance Spacy's tokenizer is rule-based and splits the text looking at the punctuation, the goal of Transformer tokenizers is to reduce the vocabulary length by splitting words into subwords. A thoroughly explanation of such these tokenizers can be found here: https://huggingface.co/docs/transformers/tokenizer_summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b60584d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1045,  2387,  1037,  2158,  2007,  1037, 12772,   102])\n",
      "['[CLS]', 'star', '##lord', 'was', 'here', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"][0])\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dcc1f",
   "metadata": {},
   "source": [
    "# Mandatory Exam Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fad3c",
   "metadata": {},
   "source": [
    "## Part 1 (4 points)\n",
    "As for LM project, you have to apply these two modifications incrementally. Also in this case you may have to play with the hyperparameters and optimizers to improve the performance. \n",
    "\n",
    "Modify the baseline architecture Model IAS by:\n",
    "- Adding bidirectionality\n",
    "- Adding dropout layer\n",
    "\n",
    "**Intent classification**: accuracy <br>\n",
    "**Slot filling**: F1 score with conll\n",
    "\n",
    "***Dataset to use: ATIS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c280e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 exercise:\n",
    "# Adding bidirectionality and dropout layer\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (output size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        # Add biodirectionality to the LSTM layer. As a result the size of the hidden states is doubled\n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=True, batch_first=True)\n",
    "        self.slot_out = nn.Linear(hid_size * 2, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size * 2, out_int)\n",
    "        # Dropout layer How/Where do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        \n",
    "        utt_emb = self.dropout(utt_emb) # we can use dropout after the embedding layer\n",
    "\n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        #.cpu().numpy() converts the seq_lengths tensor to a NumPy array and moves it to the CPU memory\n",
    "        # This is done because pack_padded_sequence expects the sequence lengths to be provided as a CPU-based NumPy array.\n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "\n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        utt_emb = self.dropout(utt_encoded) # we can use dropout after the LSTM layer too!\n",
    "        \n",
    "        # Get the last [forward and backward] hidden states\n",
    "        # Clarification: The last hidden state, obtained using last_hidden[-1, :, :], represents\n",
    "        # the hidden state corresponding to the last time step of the sequences in the batch.\n",
    "        # we concatenate the forward and backward hidden states along the hidden size dimension (dim=1)\n",
    "        # last_hidden = last_hidden[-1,:,:] # without bidirectionality\n",
    "        last_hidden = torch.cat((last_hidden[-2, :, :],  # last hidden state from the forward pass\n",
    "                                 last_hidden[-1, :, :]), # last hidden state from the backward pass\n",
    "                                 dim=1) # sequence dimension\n",
    "        \n",
    "        # Is this another possible way to get the last hiddent state? (Why?)\n",
    "        # utt_encoded.permute(1,0,2)[-1]\n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: batch_size, seq_len, classes \n",
    "        slots = slots.permute(0,2,1) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c92efa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db8d33da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 3/199 [00:07<08:07,  2.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 54\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,n_epochs)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_loop(train_loader, optimizer, criterion_slots, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                       criterion_intents, model, clip\u001b[39m=\u001b[39;49mclip)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# We check the performance every 5 epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         sampled_epochs\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 54\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                \u001b[39m# Is there another way to do that?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss_array\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# Compute the gradient, deleting the computational graph\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# clip the gradient to avoid exploding gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y104sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), clip)  \n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d5b259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y106sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m slot_f1s, intent_acc \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y106sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, runs)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y106sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     model \u001b[39m=\u001b[39m ModelIAS(hid_size, out_slot, out_int, emb_size, \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y106sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                      vocab_len, pad_index\u001b[39m=\u001b[39;49mPAD_TOKEN)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y106sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     model\u001b[39m.\u001b[39mapply(init_weights)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y106sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "n_epochs = 200\n",
    "runs = 5\n",
    "\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stopping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb2ce-9952-4127-b258-3d372f452c2b",
   "metadata": {},
   "source": [
    "## Part 2 (11 points)\n",
    "\n",
    "Adapt the code to fine-tune a pre-trained BERT model using a multi-task learning setting on intent classification and slot filling. \n",
    "You can refer to this paper to have a better understanding of how to implement this: https://arxiv.org/abs/1902.10909. In this, one of the challenges of this is to handle the sub-tokenization issue.\n",
    "\n",
    "*Note*: The fine-tuning process is to further train on a specific task/s a model that has been pre-trained on a different (potentially unrelated) task/s.\n",
    "\n",
    "\n",
    "The models that you can experiment with are [*BERT-base* or *BERT-large*](https://huggingface.co/google-bert/bert-base-uncased). \n",
    "\n",
    "**Intent classification**: accuracy <br>\n",
    "**Slot filling**: F1 score with conll\n",
    "\n",
    "***Dataset to use: ATIS***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b030c94",
   "metadata": {},
   "source": [
    "Start with some boiler plate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d369ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec88097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pprint(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9f6d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=26, bias=True)\n",
      "  (lm_head): Linear(in_features=768, out_features=130, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import nn, optim\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# Define output layers for multi-task learning\n",
    "intent_out = nn.Linear(bert_model.config.hidden_size, out_int)\n",
    "slot_out = nn.Linear(bert_model.config.hidden_size, out_slot)\n",
    "\n",
    "# Freeze BERT layers and replace top layers\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_model.classifier = intent_out\n",
    "bert_model.lm_head = slot_out\n",
    "\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69c12c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–        | 29/199 [01:13<07:11,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1:  0.04892425582080754\n",
      "Intent Accuracy: 0.7133258678611423\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss functions\n",
    "# criterion_intent = nn.CrossEntropyLoss()\n",
    "# criterion_slot = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(bert_model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 200\n",
    "patience = 5\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 5\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea4b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
