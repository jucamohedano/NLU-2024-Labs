{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca7d9e5",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "<!-- - pytorch \n",
    "    - Pytorch install: https://pytorch.org/get-started/locally/ \n",
    "- tqdm\n",
    "- sklearn\n",
    "- Huggingface Transformer: \n",
    "    - pip install transformers  -->\n",
    "- **DATASET**:\n",
    "    - https://github.com/BrownFortress/IntentSlotDatasets\n",
    "    - We will use **ATIS** only\n",
    "    \n",
    "    \n",
    "\n",
    "## Outline\n",
    "\n",
    "#### Introduction\n",
    "- sequence labelling (Slot filling)\n",
    "- text classification (Intent classification)\n",
    "\n",
    "#### Preparing text for NN\n",
    "- word2id\n",
    "- special tokens \n",
    "- Customize Dataset class\n",
    "\n",
    "#### Split data in batches\n",
    "- Usage of Dataloader class\n",
    "- Padding sequences\n",
    "\n",
    "#### Neural Networks in Pytorch\n",
    "- Word embeddings\n",
    "- Implementation of an LSTM\n",
    "- Regularization techniques\n",
    "\n",
    "#### Train and Test a Neural Network\n",
    "- Optimizer\n",
    "- Loss function\n",
    "- Iteration over batches\n",
    "\n",
    "#### Hugging face library\n",
    "- Introduction and Usage\n",
    " \n",
    " \n",
    "## References\n",
    "- RNN: https://d2l.ai/chapter_recurrent-neural-networks/index.html \n",
    "- LSTM: https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "- GRU: https://d2l.ai/chapter_recurrent-modern/gru.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0b990",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brownfortress/NLU-2024-labs/blob/main/labs/05_intent_and_slot_filling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a0554",
   "metadata": {},
   "source": [
    "# 1 Sequence Labeling,  Shallow Parsing and Text classification tasks\n",
    "\n",
    "## 1.1 Sequence Labeling and Shallow parsing\n",
    "Sequence labelling is to assign a label for each token. The task is formally defined as:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- defining a sequence of labels as $l = {l_1, l_2, ..., l_n}$\n",
    "- compute the sequence $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "A particular case of sequence labelling is [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing). The main difference from Sequence Labeling task is that Shallow Parsing performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "\n",
    "In this, we are going to see a particular case of shallow parsing task, which is named as Slot Filling (or Concept tagging). The **segmentation** part is represented with IOB tags and the **labeling** part are the concepts defined in the annotation schema of a corpus. \\\n",
    "\\\n",
    "An example is the following: \n",
    "\n",
    "| Slot Filling |  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output sequence: | O  | B-depart_date.month_name | B-depart_date.day_number | O | O    | O | O      |\n",
    "\n",
    "## 1.2 Text classification\n",
    "The text classification problem is defined  as follows:\n",
    "- Given a sequence of tokens $w = {w_1, w_2, ..., w_n}$,\n",
    "- And a set of labels $L$ where $l \\in L$\n",
    "- estimate the label $\\hat{l}$ such as $\\hat{l} = \\underset{l}{\\operatorname{argmax}} P(l|w)$ \n",
    "\n",
    "In text classification, the label is given to the whole input sequence instead of at each element of the sequence (as in sequence labelling).\n",
    "\n",
    "The text classification task that we are going to see in this laboratory is named as Intent Classification. The Intent is an additional component of the *semantic frame*. \\\n",
    "\\\n",
    "An example is the following:\n",
    "\n",
    "| Intent Classification|  |                     |                     |  |  |  |  |\n",
    "|------------------|----|--------------------------|--------------------------|---|------|---|--------|\n",
    "| Input sequence:  | on | april                    | first                    | I | want | a | flight |\n",
    "| Output label: | flight     |\n",
    "\n",
    "\n",
    "# 2 Dataset\n",
    "The dataset that we are going to use is ATIS (Airline Travel Information Systems). It is composed of transcriptions of humans asking about flight information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3289e",
   "metadata": {},
   "source": [
    "## 2.2 Load the dataset\n",
    "I have prepared a custom data structure for this dataset. The structure is the following:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "    \"utterance\": \"on april first i need a flight going from phoenix to san diego\", \n",
    "    \"slots\": \"O B-depart_date.month_name B-depart_date.day_number O O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name\", \n",
    "    \"intent\": \"flight\"\n",
    "    },\n",
    "    \"...\"\n",
    " ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70494a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Colab, run these commands\n",
    "# !wget -P dataset/ATIS https://raw.githubusercontent.com/BrownFortress/IntentSlotDatasets/main/ATIS/test.json\n",
    "# !wget -P dataset/ATIS https://raw.githubusercontent.com/BrownFortress/IntentSlotDatasets/main/ATIS/train.json\n",
    "# !wget https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/conll.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80808524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "import os\n",
    "device = 'cpu' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e1ea7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "{'intent': 'flight',\n",
      " 'slots': 'O O O O O B-fromloc.city_name O B-depart_time.time '\n",
      "          'I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O '\n",
      "          'B-arrive_time.period_of_day',\n",
      " 'utterance': 'i want to fly from boston at 838 am and arrive in denver at '\n",
      "              '1110 in the morning'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json \n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('dataset','ATIS','train.json'))\n",
    "test_raw = load_data(os.path.join('dataset','ATIS','test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "pprint(tmp_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b3f37",
   "metadata": {},
   "source": [
    "## 2.3 Create a dev set\n",
    "In the original split the development set (dev set) is missing. To train and find the best hyperparameter of our network the dev set is fundamental. Thus, we have to create it starting from the **traning** set. The dev set is usually the 10% of the dataset. \\\n",
    "Possible sampling strategies:\n",
    "* Take the last n elements of the training set.\n",
    "* Do a random sampling from the training set.\n",
    "* Do a stratified sampling from the training set using one or more criteria. (The best way)\n",
    "    * For further details look [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd6da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.4,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.4,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.0,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4480\n",
      "DEV size: 498\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# First we get the 10% of the training set, then we compute the percentage of these examples \n",
    "\n",
    "portion = 0.10\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "labels = []\n",
    "inputs = []\n",
    "mini_train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occurs only once, we put them in training\n",
    "        inputs.append(tmp_train_raw[id_y])\n",
    "        labels.append(y)\n",
    "    else:\n",
    "        mini_train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(inputs, labels, test_size=portion, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=labels)\n",
    "X_train.extend(mini_train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distributions\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'), \n",
    "pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:') \n",
    "pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bbdbe",
   "metadata": {},
   "source": [
    "## 2.3 Convert words to numbers (word2id)\n",
    "Neural Networks in Pytorch, as in other libraries, work with numbers and vectors.\n",
    "<br><br>\n",
    "\n",
    "**Exercise 1** *(10 minutes)*\n",
    "* Create a dictionary that maps the words and labels in the training set to unique  integers $\\geq$ 0, called indexes.\n",
    "    That is:\n",
    "    - One dictionary for mapping words to ids (w2id)\n",
    "    - One dictionary for mapping slot labels to ids (slot2id)\n",
    "    - One dictionary for mapping intent labels to ids (intent2id)\n",
    "\n",
    "* With w2id map the sentence in `sent` into the computed indexes.\n",
    "\n",
    "***Example:***\n",
    "```python\n",
    "dictionary = {\"from\": 2, \"Boston\":88, \"to\":105, \"Tokyo\":42}\n",
    "sent = \"from Boston to Tokyo\" \n",
    "# Output:\n",
    "[2,88,105,42]\n",
    "```\n",
    "\n",
    "We will see later how to convert these indexes into vectors (aka embeddings).\n",
    "\n",
    "*Add special tokens \"pad\" and \"unk\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea03b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "# Vocab: -1\n",
      "# Slots: 0\n",
      "# Intent: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "w2id = {'pad':PAD_TOKEN} # Pad tokens is 0 so the index count should start from 1\n",
    "slot2id = {'pad':PAD_TOKEN} # Pad tokens is 0 so the index count should start from 1\n",
    "intent2id = {}\n",
    "\n",
    "# Map the words only from the train set\n",
    "# Map slot and intent labels of train, dev and test set. 'unk' is not needed.\n",
    "sent = 'I wanna a flight from Toronto to Kuala Lumpur'\n",
    "\n",
    "mapping = [] # convert the sent into indexes using w2id\n",
    "print(mapping)\n",
    "\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be408b",
   "metadata": {},
   "source": [
    "## 2.4 Lang class\n",
    "Later we will need to convert those numbers in the original form, so we need to invert those dictionaries. We create a class named as Lang just for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c04f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77bc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute \n",
    "                                                            # the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw # We do not wat unk labels, \n",
    "                                        # however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "lang = Lang(words, intents, slots, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dbe8de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pad': 0,\n",
       " 'B-arrive_date.date_relative': 1,\n",
       " 'B-fromloc.airport_name': 2,\n",
       " 'B-or': 3,\n",
       " 'B-aircraft_code': 4,\n",
       " 'B-return_date.day_number': 5,\n",
       " 'B-arrive_time.period_mod': 6,\n",
       " 'I-today_relative': 7,\n",
       " 'I-meal_code': 8,\n",
       " 'B-class_type': 9,\n",
       " 'B-mod': 10,\n",
       " 'B-stoploc.airport_name': 11,\n",
       " 'I-fromloc.state_name': 12,\n",
       " 'B-state_name': 13,\n",
       " 'B-return_time.period_of_day': 14,\n",
       " 'B-airline_code': 15,\n",
       " 'B-fromloc.state_code': 16,\n",
       " 'I-arrive_time.start_time': 17,\n",
       " 'B-arrive_time.time': 18,\n",
       " 'I-toloc.airport_name': 19,\n",
       " 'I-flight_mod': 20,\n",
       " 'B-month_name': 21,\n",
       " 'B-flight_number': 22,\n",
       " 'I-airport_name': 23,\n",
       " 'I-flight_number': 24,\n",
       " 'B-depart_date.today_relative': 25,\n",
       " 'B-connect': 26,\n",
       " 'B-flight_days': 27,\n",
       " 'B-airport_name': 28,\n",
       " 'B-arrive_date.month_name': 29,\n",
       " 'B-toloc.city_name': 30,\n",
       " 'B-booking_class': 31,\n",
       " 'I-arrive_date.day_number': 32,\n",
       " 'B-meal': 33,\n",
       " 'B-flight_stop': 34,\n",
       " 'B-arrive_time.period_of_day': 35,\n",
       " 'B-arrive_time.start_time': 36,\n",
       " 'B-compartment': 37,\n",
       " 'I-state_name': 38,\n",
       " 'B-round_trip': 39,\n",
       " 'B-day_name': 40,\n",
       " 'B-time': 41,\n",
       " 'B-stoploc.state_code': 42,\n",
       " 'B-depart_time.time': 43,\n",
       " 'I-economy': 44,\n",
       " 'B-return_date.day_name': 45,\n",
       " 'B-arrive_time.end_time': 46,\n",
       " 'I-depart_time.time_relative': 47,\n",
       " 'B-restriction_code': 48,\n",
       " 'I-arrive_time.end_time': 49,\n",
       " 'I-fromloc.airport_name': 50,\n",
       " 'I-airline_name': 51,\n",
       " 'B-city_name': 52,\n",
       " 'B-flight_time': 53,\n",
       " 'I-stoploc.city_name': 54,\n",
       " 'B-arrive_date.day_number': 55,\n",
       " 'B-toloc.state_code': 56,\n",
       " 'B-fromloc.state_name': 57,\n",
       " 'B-period_of_day': 58,\n",
       " 'B-depart_date.day_name': 59,\n",
       " 'B-toloc.airport_name': 60,\n",
       " 'I-depart_time.end_time': 61,\n",
       " 'I-cost_relative': 62,\n",
       " 'B-fare_amount': 63,\n",
       " 'B-cost_relative': 64,\n",
       " 'B-depart_time.end_time': 65,\n",
       " 'I-restriction_code': 66,\n",
       " 'I-arrive_time.time_relative': 67,\n",
       " 'B-toloc.airport_code': 68,\n",
       " 'I-class_type': 69,\n",
       " 'I-round_trip': 70,\n",
       " 'I-mod': 71,\n",
       " 'B-stoploc.airport_code': 72,\n",
       " 'B-return_date.month_name': 73,\n",
       " 'B-fromloc.airport_code': 74,\n",
       " 'B-depart_date.month_name': 75,\n",
       " 'B-toloc.country_name': 76,\n",
       " 'I-depart_time.period_of_day': 77,\n",
       " 'B-return_time.period_mod': 78,\n",
       " 'I-depart_time.start_time': 79,\n",
       " 'B-economy': 80,\n",
       " 'B-flight_mod': 81,\n",
       " 'B-arrive_date.day_name': 82,\n",
       " 'I-flight_time': 83,\n",
       " 'I-arrive_time.period_of_day': 84,\n",
       " 'I-toloc.state_name': 85,\n",
       " 'B-depart_time.time_relative': 86,\n",
       " 'I-depart_date.today_relative': 87,\n",
       " 'B-state_code': 88,\n",
       " 'B-flight': 89,\n",
       " 'B-meal_description': 90,\n",
       " 'I-meal_description': 91,\n",
       " 'B-depart_time.start_time': 92,\n",
       " 'I-transport_type': 93,\n",
       " 'I-fromloc.city_name': 94,\n",
       " 'B-depart_date.date_relative': 95,\n",
       " 'B-arrive_date.today_relative': 96,\n",
       " 'I-return_date.day_number': 97,\n",
       " 'I-depart_time.time': 98,\n",
       " 'I-flight_stop': 99,\n",
       " 'B-time_relative': 100,\n",
       " 'O': 101,\n",
       " 'B-stoploc.city_name': 102,\n",
       " 'B-depart_time.period_of_day': 103,\n",
       " 'I-arrive_time.time': 104,\n",
       " 'B-day_number': 105,\n",
       " 'I-depart_date.day_number': 106,\n",
       " 'B-fromloc.city_name': 107,\n",
       " 'B-transport_type': 108,\n",
       " 'B-arrive_time.time_relative': 109,\n",
       " 'B-return_date.today_relative': 110,\n",
       " 'B-airline_name': 111,\n",
       " 'B-depart_time.period_mod': 112,\n",
       " 'I-toloc.city_name': 113,\n",
       " 'I-fare_basis_code': 114,\n",
       " 'I-return_date.today_relative': 115,\n",
       " 'B-depart_date.year': 116,\n",
       " 'I-depart_date.day_name': 117,\n",
       " 'I-city_name': 118,\n",
       " 'B-days_code': 119,\n",
       " 'B-airport_code': 120,\n",
       " 'B-fare_basis_code': 121,\n",
       " 'B-meal_code': 122,\n",
       " 'B-today_relative': 123,\n",
       " 'I-time': 124,\n",
       " 'B-depart_date.day_number': 125,\n",
       " 'B-return_date.date_relative': 126,\n",
       " 'I-fare_amount': 127,\n",
       " 'I-return_date.date_relative': 128,\n",
       " 'B-toloc.state_name': 129}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang.slot2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac746e",
   "metadata": {},
   "source": [
    "## 2.5 Customize the Dataset class\n",
    "In Pytorch the Dataset class helps you in handeling the dataset. The mandatory methods are ```__init__, __len__ and __getitem__```. <br>\n",
    "You can find more details here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b4823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class IntentsAndSlots (data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset class for intent classification and slot filling tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by mapping utterances, slots, and intents to integer IDs.\n",
    "\n",
    "        :param dataset: List of dictionaries, each containing 'utterance', 'slots', and 'intent'.\n",
    "        :param lang: Language object containing mapping information.\n",
    "        :param unk: Unknown token (default is 'unk').\n",
    "        \"\"\"\n",
    "\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "        \n",
    "        # Map utterances, slots, and intents to integer IDs\n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the dictionary for the example at index idx.\n",
    "\n",
    "        :param idx: Index of the example to retrieve.\n",
    "        :return: Dictionary containing 'utterance', 'slots', and 'intent' as integer IDs.\n",
    "        \"\"\"\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        \"\"\"\n",
    "        Map a list of labels to integer IDs, using the unknown token ID if not found in mapper.\n",
    "\n",
    "        :param data: List of labels.\n",
    "        :param mapper: Mapper dictionary.\n",
    "        :return: List of integer IDs.\n",
    "        \"\"\"\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        \"\"\"\n",
    "        Map a list of sequences to integer IDs, tokenizing each sequence.\n",
    "\n",
    "        :param data: List of sequences.\n",
    "        :param mapper: Mapper dictionary.\n",
    "        :return: List of tokenized and mapped sequences.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845ab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our datasets\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6630cc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': tensor([ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.]),\n",
       " 'slots': tensor([101., 101., 101., 101., 101., 101., 101., 101., 107., 101.,  30.]),\n",
       " 'intent': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297a312",
   "metadata": {},
   "source": [
    "# 3 Batches\n",
    "Batches are used to handle large datasets in the memory. Since the whole dataset cannot fit in GPU memories, we randomly shuffle the dataset and we split it in small batches that will be processed one at a time.\n",
    "## 3.1 Padding\n",
    "Padding is a strategy to fit sequences of different lengths into a matrix. For instance:\n",
    "\n",
    "| Right padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| book | me | a | flight | [pad] | [pad] | [pad] | \n",
    "\n",
    "| Left padding|   |    |   |  |  |  |  \n",
    "|---|----|---|---|---|------|---|\n",
    "| I | saw| a | unk | with | a | telescope | \n",
    "| [pad] | [pad] | [pad] | book | me | a | flight | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eb4c4",
   "metadata": {},
   "source": [
    "**Exercise 2** *(5 minutes)* <br> \n",
    "Write a function that adds padding on the right. (No need to convert the sentences to numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71bb2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split them by white space\n",
    "sequences = ['I saw a man with a telescope', \n",
    "             'book me a flight', \n",
    "             'I want to see the flights from Milan to Ibiza']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b71d2",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "To split the dataset into batches and add padding we will use the DataLoader class. \n",
    "```python\n",
    "DataLoader(Dataset, batch_size=N, collate_fn={custom function}, shuffle=True)\n",
    "```\n",
    "*collate_fn* is used to shape the output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7105180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    # print(len(data[0]))\n",
    "    # print(data[0].keys())\n",
    "    # print(data[0][\"utterance\"])\n",
    "\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "        \n",
    "    # print(len(new_item))\n",
    "    # print([d[key] for d in data])\n",
    "    # print(new_item[\"utterance\"])\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our selected device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiations\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e459effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization example for the data in the previous cell\n",
    "# For help. Right before calling merge, this is how the data dictionaries that we have look like.\n",
    "data = [\n",
    "    {'utterance': ['Hello'], 'slots': ['Greeting'], 'intent': 'greet'},\n",
    "    {'utterance': ['How are you?'], 'slots': ['Greeting'], 'intent': 'greet'},\n",
    "    {'utterance': ['Goodbye'], 'slots': ['Farewell'], 'intent': 'farewell'}\n",
    "]\n",
    "\n",
    "# After running the for loop we get:\n",
    "new_item = {\n",
    "    'utterance': [['Hello'], ['How are you?'], ['Goodbye']],\n",
    "    'slots': [['Greeting'], ['Greeting'], ['Farewell']],\n",
    "    'intent': ['greet', 'greet', 'farewell']\n",
    "}\n",
    "# then the next step would be merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019e479",
   "metadata": {},
   "source": [
    "# 4 Define a neural network in Pytorch\n",
    "In PyTorch the definition of a neural network is quite flexible. In ```__init__``` the layer that is going to be used are instantiated. In ```forward```, the architecture of the neural network is defined. Here you can find all the layers provided by Pytorch https://pytorch.org/docs/stable/nn.html while here you can find the recurrent layers https://pytorch.org/docs/stable/nn.html#recurrent-layers. \n",
    "\n",
    "<br><br>\n",
    "**pack_padded_sequence** and **pad_packed_sequences** respectively compress and uncompress sequences to remove the padding embeddings from the computation, reducing the computational cost and, therefore, the CO2 emissions.\n",
    " ![](https://i.stack.imgur.com/LPHAs.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93adc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (output size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False, batch_first=True)\n",
    "        self.slot_out = nn.Linear(hid_size, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        # Dropout layer How/Where do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        \n",
    "        utt_emb = self.dropout(utt_emb) # we can use dropout after the embedding layer\n",
    "\n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        #.cpu().numpy() converts the seq_lengths tensor to a NumPy array and moves it to the CPU memory\n",
    "        # This is done because pack_padded_sequence expects the sequence lengths to be provided as a CPU-based NumPy array.\n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "\n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        utt_emb = self.dropout(utt_encoded) # we can use dropout after the LSTM layer too!\n",
    "        \n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        \n",
    "        # Is this another possible way to get the last hiddent state? (Why?)\n",
    "        # utt_encoded.permute(1,0,2)[-1]\n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: batch_size, seq_len, classes \n",
    "        slots = slots.permute(0,2,1) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a1ecc",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Why do we pad the sequences twice? \n",
    "  - First time in the merge function above\n",
    "  - Second time in the forward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992a22c",
   "metadata": {},
   "source": [
    "## 3.1 Function to randomly initialize the weights\n",
    "This is a generic function that randomly initialize the parameters of RNN networks and linear layers. To dig deep in to this I would suggest you to look at here: https://pytorch.org/docs/master/nn.init.html \\\n",
    "\\\n",
    "*Note: In Pytorch every parameter of the network has a proper name like weight_ih, weight_hh etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f47fe3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a362fc",
   "metadata": {},
   "source": [
    "## 3.2 Training set up\n",
    "We initialize the model and we select the hyperparameters of the neural network. Futhermore, we initialize the optimizer and we select the loss function.\n",
    "- You can find further optimization algorithms here: https://pytorch.org/docs/stable/optim.html\n",
    "- and further loss functions here: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e5edf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a18f4",
   "metadata": {},
   "source": [
    "### Train Loop and Evaluation Loop\n",
    "We define two functions one for training our model and the other for evaluating it. To compute the performances on the slot filling task we will use the **conll script**, while for the intent classification task we are going to use the **classification_report**.\n",
    "\n",
    "<br>\n",
    "\n",
    "In the literature, the Intent Classification task is evaluated using accuracy as a metric. The Slot filling task is evaluated using the *conll script* which computes the performance at the chunk level and the F1 score is usually reported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef292a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### <font color='blue'>Note on gradient clipping</font>\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(...): This function clips the gradients of the model's parameters. It ensures that the norm of the gradients does not exceed the specified clip value.\n",
    "Gradient Clipping:\n",
    "For each parameter tensor in the model, the function computes the L2-norm (Euclidean norm) of the gradient.\n",
    "If the norm of the gradient exceeds the clip value, the gradient tensor is rescaled to have a norm equal to clip.\n",
    "In the provided code, torch.nn.utils.clip_grad_norm_() is used to ensure that the L2-norm of the gradients does not exceed the specified clip value, effectively limiting the maximum allowed gradient magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf6dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_loop(data, optimizer, criterion_slots, criterion_intents, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "    \n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "    \n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot \n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x] \n",
    "                           for x in torch.argmax(intents, dim=1).tolist()] \n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "            \n",
    "            # Slot inference\n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:            \n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predicts a class that is not in REF\n",
    "        print(\"Warning:\", ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "        results = {\"total\":{\"f\":0}}\n",
    "        \n",
    "    report_intent = classification_report(ref_intents, hyp_intents, \n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2eaee",
   "metadata": {},
   "source": [
    "## 3.3 Train a neural network\n",
    "We train a neural network iterating several times over the training set. \n",
    "* **epochs**: number of times in which the whole training set is seen by the network\n",
    "* **early stopping**: keeps controlled the performance of the model on the dev set and interrupts the training when the performance is getting worse\n",
    "    * **patience**: wait for a number of step before interrupting the training, even though the performance is getting worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d07777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slot F1:  0.6371823093856314\n",
      "Intent Accuracy: 0.8365061590145577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 10\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc39e9c",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "To save the model you have to save:\n",
    "- The weights of the model\n",
    "- The computed vocabularies (w2id, slot2id, intent2id)\n",
    "- The optimizer (optionally, only if you want to continue with the training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = os.path.join(\"bin\", model_name)\n",
    "# saving_object = {\"epoch\": x, \n",
    "#                  \"model\": model.state_dict(), \n",
    "#                  \"optimizer\": optimizer.state_dict(), \n",
    "#                  \"w2id\": w2id, \n",
    "#                  \"slot2id\": slot2id, \n",
    "#                  \"intent2id\": intent2id}\n",
    "# torch.save(saving_object, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1466a",
   "metadata": {},
   "source": [
    "### Plot of the train and valid losses\n",
    "One of the techniques for debugging a neural network is to check the plot of the loss. If the loss goes smoothly down then the network works correctly, otherwise a deeper analysis is needed. Furthermore, this plot can be useful for deciding the learning rate and the optimizer algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1211aab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
    "plt.title('Train and Dev Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(sampled_epochs, losses_train, label='Train loss')\n",
    "plt.plot(sampled_epochs, losses_dev, label='Dev loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df73c",
   "metadata": {},
   "source": [
    "### Multiple runs\n",
    "To have reliable results on small corpora we have to train and test the model from scratch for several times. At the end, we average the results and we compute the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f08d0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 41\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,n_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_loop(train_loader, optimizer, criterion_slots, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                       criterion_intents, model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         sampled_epochs\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 41\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m data:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# Zeroing the gradient\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     slots, intent \u001b[39m=\u001b[39m model(sample[\u001b[39m'\u001b[39;49m\u001b[39mutterances\u001b[39;49m\u001b[39m'\u001b[39;49m], sample[\u001b[39m'\u001b[39;49m\u001b[39mslots_len\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_intent \u001b[39m=\u001b[39m criterion_intents(intent, sample[\u001b[39m'\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss_slot \u001b[39m=\u001b[39m criterion_slots(slots, sample[\u001b[39m'\u001b[39m\u001b[39my_slots\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 41\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m packed_input \u001b[39m=\u001b[39m pack_padded_sequence(utt_emb, seq_lengths\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Process the batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m packed_output, (last_hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mutt_encoder(packed_input) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Unpack the sequence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#X55sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m utt_encoded, input_sizes \u001b[39m=\u001b[39m pad_packed_sequence(packed_output, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/rnn.py:881\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    878\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    879\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    880\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[1;32m    883\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[1;32m    884\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "n_epochs = 200\n",
    "runs = 5\n",
    "\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stopping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4565d",
   "metadata": {},
   "source": [
    " ![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)\n",
    "# Hugging Face\n",
    "Hugging Face is a library that allows you to use pretrained models in an easy way. This means that you do not need to implement an architecture and train it from scratch. Hugging Face is based on a community where people share trained models and code.\n",
    "<br/><br/>\n",
    "In Hugging Face there are many different models (https://huggingface.co/models) that you can import and each of them has its own input and output shapes. However, Transformer-based models are usually composed of two parts: \n",
    "- **Tokenizer**\n",
    "- **Architecture/Pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b01cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0]]),\n",
      " 'input_ids': tensor([[  101,  1045,  2387,  1037,  2158,  2007,  1037, 12772,   102],\n",
      "        [  101,  2732, 19980,  2001,  2182,   102,     0,     0,     0],\n",
      "        [  101,  1045,  2134,  1005,  1056,   102,     0,     0,     0]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# BERT model script from: huggingface.co\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Download the tokenizer\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") # Download the model\n",
    "\n",
    "inputs = tokenizer([\"I saw a man with a telescope\", \"StarLord was here\",  \"I didn't\"], return_tensors=\"pt\", padding=True)\n",
    "pprint(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f26671",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9139e1c",
   "metadata": {},
   "source": [
    "##  Byte pair encoding\n",
    "The tricky part of using Transformer-based model is the tokenizer which is based on byte pair encoding algorithm. Indeed, the **tokenizers** used by Transformer-based models are different from those we have seen in the lab. While for instance Spacy's tokenizer is rule-based and splits the text looking at the punctuation, the goal of Transformer tokenizers is to reduce the vocabulary length by splitting words into subwords. A thoroughly explanation of such these tokenizers can be found here: https://huggingface.co/docs/transformers/tokenizer_summary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b60584d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1045,  2387,  1037,  2158,  2007,  1037, 12772,   102])\n",
      "['[CLS]', 'i', 'saw', 'a', 'man', 'with', 'a', 'telescope', '[SEP]']\n",
      "['[CLS]', 'star', '##lord', 'was', 'here', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"][0])\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2dcc1f",
   "metadata": {},
   "source": [
    "# Mandatory Exam Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fad3c",
   "metadata": {},
   "source": [
    "## Part 1 (4 points)\n",
    "As for LM project, you have to apply these two modifications incrementally. Also in this case you may have to play with the hyperparameters and optimizers to improve the performance. \n",
    "\n",
    "Modify the baseline architecture Model IAS by:\n",
    "- Adding bidirectionality\n",
    "- Adding dropout layer\n",
    "\n",
    "**Intent classification**: accuracy <br>\n",
    "**Slot filling**: F1 score with conll\n",
    "\n",
    "***Dataset to use: ATIS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c280e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 exercise:\n",
    "# Adding bidirectionality and dropout layer\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=1, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (output size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "        \n",
    "        # Add biodirectionality to the LSTM layer. As a result the size of the hidden states is doubled\n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=True, batch_first=True)\n",
    "        self.slot_out = nn.Linear(hid_size * 2, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size * 2, out_int)\n",
    "        # Dropout layer How/Where do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        \n",
    "        utt_emb = self.dropout(utt_emb) # we can use dropout after the embedding layer\n",
    "\n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "        #.cpu().numpy() converts the seq_lengths tensor to a NumPy array and moves it to the CPU memory\n",
    "        # This is done because pack_padded_sequence expects the sequence lengths to be provided as a CPU-based NumPy array.\n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input) \n",
    "\n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        utt_emb = self.dropout(utt_encoded) # we can use dropout after the LSTM layer too!\n",
    "        \n",
    "        # Get the last [forward and backward] hidden states\n",
    "        # Clarification: The last hidden state, obtained using last_hidden[-1, :, :], represents\n",
    "        # the hidden state corresponding to the last time step of the sequences in the batch.\n",
    "        # we concatenate the forward and backward hidden states along the hidden size dimension (dim=1)\n",
    "        # last_hidden = last_hidden[-1,:,:] # without bidirectionality\n",
    "        last_hidden = torch.cat((last_hidden[-2, :, :],  # last hidden state from the forward pass\n",
    "                                 last_hidden[-1, :, :]), # last hidden state from the backward pass\n",
    "                                 dim=1) # sequence dimension\n",
    "        \n",
    "        # Is this another possible way to get the last hiddent state? (Why?)\n",
    "        # utt_encoded.permute(1,0,2)[-1]\n",
    "        \n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        \n",
    "        # Slot size: batch_size, seq_len, classes \n",
    "        slots = slots.permute(0,2,1) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c92efa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "n_epochs = 200\n",
    "runs = 5\n",
    "\n",
    "slot_f1s, intent_acc = [], []\n",
    "for x in tqdm(range(0, runs)):\n",
    "    model = ModelIAS(hid_size, out_slot, out_int, emb_size, \n",
    "                     vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    criterion_intents = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    patience = 3\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    sampled_epochs = []\n",
    "    best_f1 = 0\n",
    "    for x in range(1,n_epochs):\n",
    "        loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                          criterion_intents, model)\n",
    "        if x % 5 == 0:\n",
    "            sampled_epochs.append(x)\n",
    "            losses_train.append(np.asarray(loss).mean())\n",
    "            results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                          criterion_intents, model, lang)\n",
    "            losses_dev.append(np.asarray(loss_dev).mean())\n",
    "            f1 = results_dev['total']['f']\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if patience <= 0: # Early stopping with patient\n",
    "                break # Not nice but it keeps the code clean\n",
    "\n",
    "    results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                             criterion_intents, model, lang)\n",
    "    intent_acc.append(intent_test['accuracy'])\n",
    "    slot_f1s.append(results_test['total']['f'])\n",
    "slot_f1s = np.asarray(slot_f1s)\n",
    "intent_acc = np.asarray(intent_acc)\n",
    "print('Slot F1', round(slot_f1s.mean(),3), '+-', round(slot_f1s.std(),3))\n",
    "print('Intent Acc', round(intent_acc.mean(), 3), '+-', round(slot_f1s.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb2ce-9952-4127-b258-3d372f452c2b",
   "metadata": {},
   "source": [
    "## Part 2 (11 points)\n",
    "\n",
    "Adapt the code to fine-tune a pre-trained BERT model using a multi-task learning setting on intent classification and slot filling. \n",
    "You can refer to this paper to have a better understanding of how to implement this: https://arxiv.org/abs/1902.10909. In this, one of the challenges of this is to handle the sub-tokenization issue.\n",
    "\n",
    "*Note*: The fine-tuning process is to further train on a specific task/s a model that has been pre-trained on a different (potentially unrelated) task/s.\n",
    "\n",
    "\n",
    "The models that you can experiment with are [*BERT-base* or *BERT-large*](https://huggingface.co/google-bert/bert-base-uncased). \n",
    "\n",
    "**Intent classification**: accuracy <br>\n",
    "**Slot filling**: F1 score with conll\n",
    "\n",
    "***Dataset to use: ATIS***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b030c94",
   "metadata": {},
   "source": [
    "Start with some boiler plate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d369ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad716661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'utterance': 'what is the cost for these flights from baltimore to philadelphia',\n",
       "  'slots': 'O O O O O O O O B-fromloc.city_name O B-toloc.city_name',\n",
       "  'intent': 'airfare'},\n",
       " {'utterance': 'flights from westchester county to san francisco daily',\n",
       "  'slots': 'O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name I-toloc.city_name B-flight_days',\n",
       "  'intent': 'flight'},\n",
       " {'utterance': 'i would like a flight from philadelphia to dallas on american airlines',\n",
       "  'slots': 'O O O O O O B-fromloc.city_name O B-toloc.city_name O B-airline_name I-airline_name',\n",
       "  'intent': 'flight'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[:3]\n",
    "# dev_raw\n",
    "# test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10674060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert uses a a wordpiece tokenizer\n",
    "# We are labeling whole words. We have to change this to only label the first token produced after tokenizing a word\n",
    "# and the rest of the tokens that form a word will labeled with the padding token\n",
    "\n",
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        # self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "        \n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements) # counter returns a dict with {k:<number of occurrences of k>}\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2f3cb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5dd2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BertIntentsAndSlots(data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset class for intent classification and slot filling tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by mapping utterances, slots, and intents to integer IDs.\n",
    "\n",
    "        :param dataset: List of dictionaries, each containing 'utterance', 'slots', and 'intent'.\n",
    "        :param lang: Language object containing mapping information.\n",
    "        :param unk: Unknown token (default is 'unk').\n",
    "        \"\"\"\n",
    "\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        \n",
    "        # Map utterances, slots, and intents to integer IDs\n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "        # self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the dictionary for the example at index idx.\n",
    "\n",
    "        :param idx: Index of the example to retrieve.\n",
    "        :return: Dictionary containing 'utterance', 'slots', and 'intent' as integer IDs.\n",
    "        \"\"\"\n",
    "        utt = self.utterances[idx]\n",
    "        slots = self.slot_ids[idx]\n",
    "        intent = self.intent_ids[idx]\n",
    "\n",
    "        utt_inputs = tokenizer(utt, \n",
    "                               return_tensors=\"pt\", \n",
    "                               padding=True,\n",
    "                               max_length=200,\n",
    "                               pad_to_max_length=True,\n",
    "                               return_token_type_ids=True)\n",
    "        \n",
    "        # Tokenize word by word (for NER)\n",
    "        # for w in zip(utt, slots):\n",
    "\n",
    "        \n",
    "        ids = utt_inputs['input_ids']\n",
    "        mask = utt_inputs['attention_mask']\n",
    "        token_type_ids = utt_inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'slots' : torch.tensor(slots, dtype=torch.long).unsqueeze(0), # unsqueeze so that we add an extra dim so that we can take .shape[1] in merge function\n",
    "            'intent': intent\n",
    "        }\n",
    "    \n",
    "    def mapping_lab(self, data, mapper):\n",
    "        \"\"\"\n",
    "        Map a list of labels to integer IDs, using the unknown token ID if not found in mapper.\n",
    "\n",
    "        :param data: List of labels.\n",
    "        :param mapper: Mapper dictionary.\n",
    "        :return: List of integer IDs.\n",
    "        \"\"\"\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "    \n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        \"\"\"\n",
    "        Map a list of sequences to integer IDs, tokenizing each sequence.\n",
    "\n",
    "        :param data: List of sequences.\n",
    "        :param mapper: Mapper dictionary.\n",
    "        :return: List of tokenized and mapped sequences.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for seq, utt in zip(data, self.utterances):\n",
    "            tmp_seq = []\n",
    "            # example of slot: print(seq)= O O O O O O O O B-fromloc.city_name O B-toloc.city_name\n",
    "            for x, w in zip(seq.split(), utt.split()):\n",
    "                tokenized_word = tokenizer(w)['input_ids']\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append([mapper[x]] + [PAD_TOKEN]*(len(tokenized_word)-1)) # Example: [ [ID][PAD][PAD][PAD] ]\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "536b67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BertIntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = BertIntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = BertIntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a92e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0][\"slots\"])\n",
    "tokenizer.convert_ids_to_tokens(train_dataset[4][\"ids\"][0]), train_raw[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812029a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec30b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e365a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8944b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816cf284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb046de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "b5d30354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [seq.shape[1] for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['ids']), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "        \n",
    "    # print(len(new_item))\n",
    "    # print([d[key] for d in data])\n",
    "    # print(new_item[\"utterance\"])\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['ids'])\n",
    "    mask, _ = merge(new_item['mask'])\n",
    "    token_type_ids, _ = merge(new_item['mask'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "    print(y_slots.shape)\n",
    "    \n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our selected device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "    \n",
    "    new_item[\"ids\"] = src_utt\n",
    "    new_item[\"mask\"] = mask\n",
    "    new_item[\"token_type_ids\"] = token_type_ids\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "e156c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     Collate function for DataLoader.\n",
    "#     \"\"\"\n",
    "#     def merge(sequences):\n",
    "#         '''\n",
    "#         merge from batch * sent_len to batch * max_len \n",
    "#         '''\n",
    "#         lengths = [len(seq) for seq in sequences]\n",
    "#         max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "#         # Pad token is zero in our case\n",
    "#         # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "#         # batch_size X maximum length of a sequence\n",
    "#         padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "#         for i, seq in enumerate(sequences):\n",
    "#             print(seq)\n",
    "#             end = lengths[i]\n",
    "#             padded_seqs[i, :end] = seq[\"ids\"] # We copy each sequence into the matrix\n",
    "#         # print(padded_seqs)\n",
    "#         padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "#         return padded_seqs, lengths\n",
    "#     # Sort data by sequence lengths in descending order\n",
    "#     data.sort(key=lambda x: len(x['ids']), reverse=True) \n",
    "\n",
    "#     # Merge sequences and their lengths\n",
    "#     src_utt, seq_lengths = merge(data)\n",
    "\n",
    "#     # Pad sequences to the length of the longest sequence in the batch\n",
    "#     max_len = max(seq_lengths)\n",
    "#     padded_seqs = torch.zeros((len(data), max_len), dtype=torch.long, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "#     padded_seqs.fill_(PAD_TOKEN)\n",
    "\n",
    "#     # Copy each sequence into the padded tensor\n",
    "#     for i, seq in enumerate(data):\n",
    "#         end = seq_lengths[i]\n",
    "#         padded_seqs[i, :end] = seq['utterance']  # Assign sequence to padded tensor\n",
    "#         seq_lengths[i] = end  # Assign sequence length to lengths tensor\n",
    "\n",
    "#     # Detach the padded tensor from the computational graph\n",
    "#     padded_seqs = padded_seqs.detach()\n",
    "\n",
    "#     # Convert other data to tensors\n",
    "#     y_slots = torch.stack([torch.tensor(x, dtype=torch.long) for x in data['slots']])\n",
    "#     y_intents = torch.tensor(data['intents'], dtype=torch.long)\n",
    "#     y_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
    "\n",
    "#     # Return a dictionary with all the necessary tensors\n",
    "#     return {'ids': padded_seqs, 'mask': y_lengths, 'slots': y_slots, 'intents': y_intents}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "8d911471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juancm/miniconda3/envs/nlu24/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2663: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/juancm/miniconda3/envs/nlu24/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2668: UserWarning: Though `pad_to_max_length` = `True`, it is ignored because `padding`=`True`.\n",
      "  warnings.warn(\"Though `pad_to_max_length` = `True`, it is ignored because `padding`=`True`.\")\n",
      "/tmp/ipykernel_38110/1195358988.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(ids, dtype=torch.long),\n",
      "/tmp/ipykernel_38110/1195358988.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(mask, dtype=torch.long),\n",
      "/tmp/ipykernel_38110/1195358988.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[ 101, 2054, 2003, 1996, 3465, 2005, 2122, 7599, 2013, 6222, 2000, 4407,\n",
       "           102]]),\n",
       " 'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'slots': tensor([[41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2]]),\n",
       " 'intent': 2}"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "f0e9d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "86b96831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38110/1195358988.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(ids, dtype=torch.long),\n",
      "/tmp/ipykernel_38110/1195358988.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(mask, dtype=torch.long),\n",
      "/tmp/ipykernel_38110/1195358988.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[\"ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "e9f6d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=26, bias=True)\n",
      "  (lm_head): Linear(in_features=768, out_features=130, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define output layers for multi-task learning\n",
    "intent_out = nn.Linear(bert_model.config.hidden_size, out_int)\n",
    "slot_out = nn.Linear(bert_model.config.hidden_size, out_slot)\n",
    "\n",
    "# Freeze BERT layers and replace top layers\n",
    "# for param in bert_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "bert_model.classifier = intent_out\n",
    "bert_model.lm_head = slot_out\n",
    "\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "469f7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, optimizer, criterion_slots, criterion_intents, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        print(sample)\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['ids'], attention_mask=sample['mask'], token_type_ids=sample['token_type_ids'], return_dict=False)\n",
    "        # _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses. \n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "69c12c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/299 [00:00<?, ?it/s]/tmp/ipykernel_38110/1195358988.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(ids, dtype=torch.long),\n",
      "/tmp/ipykernel_38110/1195358988.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(mask, dtype=torch.long),\n",
      "/tmp/ipykernel_38110/1195358988.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': tensor([[ 101, 2265, 2033,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2052,  ...,    0,    0,    0],\n",
      "        [ 101, 2129, 2521,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2862, 2035,  ...,    0,    0,    0],\n",
      "        [ 101, 3531, 2265,  ...,    0,    0,    0],\n",
      "        [ 101, 2034, 2465,  ...,    0,    0,    0]]), 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'slots': [tensor([[41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41,  41,  41,  41, 101,  41,  41,  62,  41,  41,  41,  41,\n",
      "          15,  41,   2,  74,  74,  41,  41,  41,  72]]), tensor([[41, 41, 41, 41, 41, 41, 15, 53]]), tensor([[ 41,  41,  41, 101,  41,  41,  15,  53,  41,   2]]), tensor([[41, 41, 41, 41, 41, 41, 41, 45]]), tensor([[ 41,  99,  56,  41,  41, 118,  41,   2]]), tensor([[41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 76]]), tensor([[ 41,  41, 102,  41,  41,  99,  56,  41,  15,  53,  41,   2]]), tensor([[41, 41, 41, 41, 41,  2, 41, 15, 85, 95, 41, 42,  8]]), tensor([[41, 41, 41, 41, 41, 41, 45, 11]]), tensor([[ 41,  41,  41,  41,  41, 117,  86,  41,  41,  15,  41,   2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 74, 74]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 15, 41,  2, 74, 41, 62]]), tensor([[41, 41, 41, 41, 41, 41, 41, 45]]), tensor([[41, 41, 41, 41, 41, 15, 53, 53, 41,  2, 74, 41, 41, 41, 41, 15, 41,  2,\n",
      "         74, 41, 41, 41, 41, 41, 41]]), tensor([[ 41,  41,  41,  41,  41,  15,  41,   2,  41,  41,  41,  41,  77,   4,\n",
      "           4,   4,  41,  41, 102,  65,  91,  79]]), tensor([[41, 41, 41, 41, 21, 41, 41, 15, 41,  2, 41, 41, 85, 95, 94]]), tensor([[41, 41, 41, 99, 41, 41,  2]]), tensor([[41, 41, 15, 41,  2, 41, 99, 56]]), tensor([[ 41,  15, 118,  41,   2,  41,  41,  41, 102]]), tensor([[ 41,  41,  41, 101,  41,  41,  15,  41,   2]]), tensor([[ 41,  41,  41,  41, 115,  29,  41,  41,  75, 120,  41,  41,  41,  41,\n",
      "          41,  41,  41]]), tensor([[41, 41, 45]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2, 41]]), tensor([[41, 41, 41, 15, 53, 41,  2, 44, 62]]), tensor([[41, 41, 41, 41, 41, 41, 45, 89]]), tensor([[99, 41, 41, 15, 53, 41]]), tensor([[41, 41, 41, 41, 41, 68, 41, 41, 99, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 15, 41,  2, 41, 41, 90, 41, 46, 50]]), tensor([[15, 41,  2, 62]]), tensor([[ 41,  41,  41,  41,  76,  41,  41,  15,  53,  41,   2,  41,  41,  41,\n",
      "         127]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41,  41, 117,  86,  41,  41,  15,  41,   2]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  41,  15,  41,  62, 102,  41,  41,\n",
      "          41,   2]]), tensor([[ 41,  41,  41,  41,  41, 102, 114,  41,  41,  15,  53,  41,   2]]), tensor([[15, 41,  2]]), tensor([[41, 41, 41, 90, 41, 41, 15, 53, 41,  2]]), tensor([[41, 41, 41, 41, 65, 91, 79, 41, 62, 41, 15, 41,  2, 74, 74]]), tensor([[41, 41, 41, 41, 15, 41,  2, 41]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  21,  41,  15, 118,  41,   2,  74,\n",
      "         101,  32,  32]]), tensor([[41, 41, 41, 90, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41, 117,  86,  88,  41,  41,  15,  41,   2]]), tensor([[41, 41, 41, 41, 41, 41, 15, 41,  2, 41, 41, 41, 41, 62]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  41,  41, 113,   1,   1,   1]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  15,  41,   2,  41,  84,  62, 102]]), tensor([[41, 41, 99, 56, 41, 41, 41, 15]]), tensor([[ 41,  41,  41,  41, 102, 102, 101,  41,  41,  15,  53,  41,   2]]), tensor([[41, 41, 15, 41,  2, 41, 62, 41, 99]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  41,  41,  41,  41,  41,  41,  41,\n",
      "          39,  63,  41,  15,  41,   2,  84,  41,  41, 102]]), tensor([[41, 41, 41, 41, 41, 41, 62, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 99, 41, 15, 41,  2]]), tensor([[ 41,  41,  41,  90, 117,  86,  41,  41,  15,  41,   2]]), tensor([[41, 41, 41, 41, 41, 15, 53, 53, 41, 34,  2, 41, 39, 63, 20]]), tensor([[ 41,  41,  41, 115,  29,  41,  41,  45]]), tensor([[ 41,  41,  41, 102,  41,  41,  15,  41,   2]]), tensor([[41, 41, 41, 41, 15, 53, 41,  2, 41, 39, 63]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 74, 41, 62, 39, 63]]), tensor([[41, 41, 41, 41, 15, 41,  2, 74]]), tensor([[41, 41, 41, 41, 21, 41, 41, 15, 53, 41,  2, 74, 76]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,  2, 41, 15, 41, 44, 41, 62]]), tensor([[41, 88, 55, 41, 41,  2, 74, 41, 99, 56, 62, 44, 41]]), tensor([[41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 41, 41, 15, 41,  2, 74, 41, 41, 41, 28]]), tensor([[41, 41, 41, 41, 41, 41, 41, 15, 53, 41, 41]]), tensor([[41, 41, 41, 41, 41, 76, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 74]]), tensor([[ 41,  41,  41,  41,  41,  41,  41, 113,   1]]), tensor([[ 41,  41,  41,  41,  41,  41, 117,  86,  41,  41,  15,  41,   2,  74]]), tensor([[41, 41, 41, 41, 41, 68, 41, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  15,  41,   2,  41,  62, 102]]), tensor([[41, 41, 41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 41, 41]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2, 74, 41, 39, 63, 20]]), tensor([[41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 41, 15, 53, 41,  2, 34]]), tensor([[41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 15, 53, 41,  2]]), tensor([[41, 41, 88, 55, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41,  41, 117,  86,  41,  41,  15,  53,  41,   2]]), tensor([[ 41,  41,  41,  41,  41,  59, 105, 105]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 63, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  15,  41,   2,  41,  41,  41,  41,\n",
      "         102,   9, 102]]), tensor([[ 41,  41,  41,  41,  41,  41,  15, 118,  41,   2,  74,  80]]), tensor([[ 41,  41,  41,  41,  41,  15,  53,  41,   2, 117,  86,  41,  62,  39,\n",
      "          63,  41,  41,   2,  41,  85,  95,  94,  41,  41, 127,  41,  41,  41,\n",
      "          87,  41,  41,  41,   2,  74,  41,  85,  95,  94,  41,  41, 127,  41,\n",
      "          41,  41,  41,  41]]), tensor([[41, 41, 41, 41, 41, 48, 41]]), tensor([[41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 41, 39, 63]]), tensor([[41, 41, 41, 90, 41, 41, 15, 53, 41,  2]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[ 41,  41,  41, 101,  41,  41,  41,  41,  41,  15,  41,   2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 41, 62,  9, 62]]), tensor([[15, 41,  2, 62]]), tensor([[ 41,  41,  41,  39,  63,  20, 122,  41,  15,  41,   2,  74]]), tensor([[ 41,  41,  41,  41,  15,  41,   2,  41,  62, 102]]), tensor([[41, 41, 41, 41, 41, 15, 53, 41,  2, 41, 99, 56]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 15, 41,  2, 74, 41, 99, 56, 41, 41, 41,\n",
      "         41]]), tensor([[41, 41, 41, 45]]), tensor([[41, 41, 41, 41, 41, 41, 41, 15, 41,  2, 74, 41, 62, 39, 63]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 41, 62, 41, 99]]), tensor([[41, 41, 41, 41, 41, 41, 41, 41, 45]]), tensor([[ 41,  41,  41,  41,  41,  41,  15,  41,   2,  74,  41,  15,  65,  91,\n",
      "          41,  41, 102,  44,  62]]), tensor([[41, 41, 41, 41, 41, 15, 41,  2, 41, 41, 85, 95, 94]]), tensor([[ 41,  77,  41,  41, 102, 114,  41,  41,  41,  15,  41,   2]]), tensor([[ 41,  41,  41,  41,  41,  15,  41,   2,  41,  41,  41,   2, 108,  85,\n",
      "          95]]), tensor([[41, 41, 15, 41,  2, 41]]), tensor([[41, 45, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41]]), tensor([[ 41,  41,  76,  41,  41,  15,  41,   2,  41,  41,  41, 127,  41,  41,\n",
      "          85,  95,  94]]), tensor([[ 41,  41,  41,  41, 101,  41,  54,  41,  15,  53,  41,   2]]), tensor([[41, 41, 41, 41, 41, 15, 53, 41,  2, 74, 41, 41, 41, 28]]), tensor([[41, 41, 41, 41, 41, 15, 26, 41,  2, 74, 80, 41, 44, 62]]), tensor([[ 41,  41,  41, 117,  86,  41,  41,  15,  53,  41,   2,  76]]), tensor([[ 41,  41,  41,  41,  41,  41,  41,  15,  26,  41,   2,  41,  41,  62,\n",
      "         102]]), tensor([[41, 41, 99, 56, 41, 41, 15, 39, 63]]), tensor([[41, 41, 41, 41, 41, 41, 45]]), tensor([[41, 41, 41, 41, 41, 41, 41, 15, 41,  2, 41, 62, 41, 41, 41,  2, 85, 95,\n",
      "         94]]), tensor([[ 41,  41,  41,  41, 117,  86,  41,  41,  15,  41,   2]]), tensor([[41, 41, 41, 15, 41, 41, 41, 41, 41, 41, 41, 41, 41,  2]]), tensor([[ 41,  41,  41,  41,  41, 113,   1]]), tensor([[ 41,  41,  41,  41, 117,  41,  41,  15,  41,   2,  74]]), tensor([[ 88,  55,  41,  15,  53,  15, 117,  86]])], 'intent': [14, 14, 20, 14, 8, 14, 14, 14, 14, 14, 14, 8, 14, 14, 14, 14, 0, 14, 14, 14, 14, 14, 14, 14, 8, 8, 14, 14, 14, 8, 14, 14, 2, 14, 14, 14, 14, 2, 14, 14, 14, 14, 14, 14, 14, 14, 2, 14, 14, 14, 19, 14, 14, 14, 14, 14, 14, 14, 8, 14, 14, 14, 14, 14, 14, 14, 14, 14, 8, 14, 14, 8, 2, 2, 14, 14, 14, 14, 14, 11, 14, 14, 14, 14, 14, 14, 18, 14, 14, 14, 14, 4, 14, 14, 2, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0, 14, 8, 14, 14, 17, 14, 14, 8, 14, 14, 14, 14, 2, 14, 19, 8, 14, 2, 14, 14, 14, 2], 'intents': tensor([14, 14, 20, 14,  8, 14, 14, 14, 14, 14, 14,  8, 14, 14, 14, 14,  0, 14,\n",
      "        14, 14, 14, 14, 14, 14,  8,  8, 14, 14, 14,  8, 14, 14,  2, 14, 14, 14,\n",
      "        14,  2, 14, 14, 14, 14, 14, 14, 14, 14,  2, 14, 14, 14, 19, 14, 14, 14,\n",
      "        14, 14, 14, 14,  8, 14, 14, 14, 14, 14, 14, 14, 14, 14,  8, 14, 14,  8,\n",
      "         2,  2, 14, 14, 14, 14, 14, 11, 14, 14, 14, 14, 14, 14, 18, 14, 14, 14,\n",
      "        14,  4, 14, 14,  2, 14, 14, 14, 14, 14, 14, 14, 14, 14,  0,  0, 14,  8,\n",
      "        14, 14, 17, 14, 14,  8, 14, 14, 14, 14,  2, 14, 19,  8, 14,  2, 14, 14,\n",
      "        14,  2]), 'y_slots': tensor([[41, 41, 41,  ...,  0,  0,  0],\n",
      "        [41, 41, 41,  ...,  0,  0,  0],\n",
      "        [41, 41, 41,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [41, 41, 41,  ...,  0,  0,  0],\n",
      "        [41, 41, 41,  ...,  0,  0,  0],\n",
      "        [88, 55, 41,  ...,  0,  0,  0]]), 'slots_len': tensor([ 8, 23,  8, 10,  8,  8,  7, 10,  9, 12, 13,  8, 12, 10, 11, 10,  8, 25,\n",
      "        22, 15,  7,  8,  9,  9, 17,  3,  8, 12,  9,  8,  6, 13, 13,  4, 15,  8,\n",
      "         7, 11, 16, 13,  3, 10, 15,  8, 17,  9, 11, 14, 13, 14,  8, 13,  9, 24,\n",
      "        11, 10, 11, 15,  8,  9, 11, 13,  8, 13, 18, 13,  6, 15, 11, 10,  9,  9,\n",
      "        14, 12, 13, 10, 15,  7, 10, 17,  7, 11,  6,  8,  9, 12,  8, 16, 17, 12,\n",
      "        46,  7,  6, 11, 10, 12, 12,  8, 12,  4, 12, 10, 12, 19,  4, 15, 12,  9,\n",
      "        19, 13, 12, 15,  6, 14, 17, 12, 14, 14, 12, 15,  9,  7, 19, 11, 14,  7,\n",
      "        11,  8])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/299 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [128, 768], got [128, 46]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,n_epochs)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_loop(train_loader, optimizer, criterion_slots, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                       criterion_intents, bert_model, clip\u001b[39m=\u001b[39;49mclip)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# We check the performance every 5 epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         sampled_epochs\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[1;32m/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss_intent \u001b[39m=\u001b[39m criterion_intents(intent, sample[\u001b[39m'\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss_slot \u001b[39m=\u001b[39m criterion_slots(slots, sample[\u001b[39m'\u001b[39;49m\u001b[39my_slots\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_intent \u001b[39m+\u001b[39m loss_slot \u001b[39m# In joint training we sum the losses. \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/juancm/trento/nlu/my_fork/NLU-2024-Labs/labs/05_intent_and_slot_filling.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                \u001b[39m# Is there another way to do that?\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlu24/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [128, 768], got [128, 46]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(bert_model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 300\n",
    "patience = 5\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots, \n",
    "                      criterion_intents, bert_model, clip=clip)\n",
    "    if x % 5 == 0: # We check the performance every 5 epochs\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots, \n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        \n",
    "        f1 = results_dev['total']['f']\n",
    "        # For decreasing the patience you can also use the average between slot f1 and intent accuracy\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            # Here you should save the model\n",
    "            patience = 5\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots, \n",
    "                                         criterion_intents, model, lang)    \n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6fea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
